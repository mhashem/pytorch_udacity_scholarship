
# Lesson 2

### Softmax
Any time we wish to represent a probability distribution over a discrete variable with **n** possible values, we may use the softmax function. This can be seen as a generalization of the sigmoid function which was used to represent a probability distribution over a binary variable.

Softmax functions are most often used as the output of a classifier, to represent the probability distribution over **n** different classes. More rarely, softmax functions can be used inside the model itself, if we wish the model to choose between one of
**n** different options for some internal variable.

### One-hot encoding

### Maximum Likelihood

### Cross Entropy

### Gradient Descent

Most deep learning algorithms involve optimization of some sort. Optimization refers to the task of either minimizing or maximizing some function ***f(x)*** by altering ***x***.



